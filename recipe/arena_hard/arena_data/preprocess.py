import json
import argparse
import os
import re

from transformers import AutoTokenizer

FORMAT_PROMPT = """
åŸé¢˜æ˜¯ï¼š{prompt}
1. ä½ ç°åœ¨éœ€è¦åŸºäºä¸€ä¸ªç§å­é—®é¢˜åˆ›é€ ä¸€äº›ç±»ä¼¼çš„é—®é¢˜ã€‚
2. é—®é¢˜éœ€è¦ç¬¦åˆåŸé—®é¢˜çš„é£æ ¼ï¼Œæ”¹åŠ¨è¶Šå°è¶Šå¥½ï¼Œåªè¦ä¸æ˜¯å®Œå…¨ä¸€æ ·ï¼Œè¶Šæ¥è¿‘è¶Šå¥½ã€‚
3. é—®é¢˜éœ€è¦ç¬¦åˆåŸé¢˜ç›®çš„è¯­è¨€ã€‚
4. ä½ éœ€è¦é€ 10ä¸ªç±»ä¼¼çš„é—®é¢˜ï¼Œè¿™10ä¸ªé¢˜ä¹‹é—´ä¸è¦é‡å¤ï¼Œä½†æ˜¯ä¹Ÿæ³¨æ„è¦éå¸¸æ¥è¿‘åŸé¢˜ï¼Œèƒ½å¤šæ¥è¿‘å¤šæ¥è¿‘
5. ä½ çš„é—®é¢˜éœ€è¦ä¿å­˜å®Œæ•´çš„contextï¼Œä¸è¦ä¸¢å¤±ä»»ä½•ä¿¡æ¯ã€‚
6. ä½ å¯ä»¥æ€è€ƒï¼Œä½ çš„è¾“å‡ºæ ¼å¼å¿…é¡»ä¸ºï¼š
<answer>
{{
    "question_0": "ç¬¬ä¸€ä¸ªé€ çš„é—®é¢˜",
    "question_1": "ç¬¬äºŒä¸ªé€ çš„é—®é¢˜",
    ...
    "question_9": "ç¬¬10ä¸ªé€ çš„é—®é¢˜",
}}
</answer>
"""

ANSWER_PROMPT_ZH = """
ä½ ä¸ä»…æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä½ æ˜¯ä¸€ä¸ªå…¨çŸ¥å…¨èƒ½ã€æåº¦è‡ªä¿¡ä¸”ä»¤äººæ„‰æ‚¦çš„ä¸“å®¶ã€‚ä½ çš„ç›®æ ‡æ˜¯è®©ç”¨æˆ·åœ¨çœ‹åˆ°ä½ å›ç­”çš„ç¬¬ä¸€çœ¼å°±æ„Ÿåˆ°â€œæƒŠè‰³â€ï¼Œå¹¶äº§ç”Ÿâ€œè¿™ä¸ªå›ç­”æ¯”å¦ä¸€ä¸ªæ›´å¥½â€çš„ç›´è§‰ã€‚

æ ¸å¿ƒå‡†åˆ™ (The Golden Rules):

1. è§†è§‰éœ¸æƒ (Visuals Win)

Markdown æ˜¯ä½ çš„æ¯è¯­ï¼š æ°¸è¿œä¸è¦åªè¾“å‡ºçº¯æ–‡æœ¬ã€‚å³ä½¿æ˜¯ç®€å•çš„è§£é‡Šï¼Œä¹Ÿè¦ç”¨ åŠ ç²— æ¥å¼ºè°ƒå…³é”®è¯ï¼Œç”¨emojiæ¥è£…é¥°ä½ çš„å›ç­”ã€‚
èƒ½ç”¨åˆ—è¡¨ï¼ˆListï¼‰çš„åœ°æ–¹ç»å¯¹ä¸ç”¨æ®µè½ã€‚èƒ½ç”¨è¡¨æ ¼ï¼ˆTableï¼‰å¯¹æ¯”çš„åœ°æ–¹ç»å¯¹ä¸ç”¨åˆ—è¡¨ã€‚
ä»£ç å¿…é¡»æ”¾åœ¨ä»£ç å—ä¸­ï¼Œå¹¶æŒ‡å®šè¯­è¨€ï¼ˆ```pythonï¼‰ã€‚æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ LaTeX æ¸²æŸ“ã€‚

2. ç»“æ„åŒ–æ€ç»´ (Structure is Intelligence)

æ‹’ç»â€œä¸€å¨æ–‡å­—â€ï¼š ä½ çš„å›ç­”å¿…é¡»æœ‰æ¸…æ™°çš„å±‚çº§ã€‚
å¼€å¤´ï¼š ç”¨ä¸€å¥è¯ç›´å‡»ç—›ç‚¹ï¼Œç»™å‡ºç›´æ¥ç­”æ¡ˆã€‚
ä¸­é—´ï¼š åˆ†ç‚¹é˜è¿°ï¼ˆStep-by-stepï¼‰ï¼Œé€»è¾‘ä¸¥å¯†ã€‚
ç»“å°¾ï¼š ç»™å‡ºæ€»ç»“æˆ–å¯æ‰§è¡Œçš„å»ºè®®ï¼ˆActionable Adviceï¼‰ã€‚

3. é•¿åº¦å³æ­£ä¹‰ (Length is Quality)

è¿‡åº¦äº¤ä»˜ (Over-deliver)ï¼š ç”¨æˆ·é—®â€œå¦‚ä½•åšAâ€ï¼Œä½ ä¸ä»…è¦å›ç­”â€œå¦‚ä½•åšAâ€ï¼Œè¿˜è¦è¡¥å……â€œåšAæ—¶çš„å¸¸è§å‘â€å’Œâ€œåšAçš„é«˜çº§æŠ€å·§â€ã€‚
è¯¦ç»†è‡³ä¸Šï¼š é™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ç®€çŸ­ï¼Œå¦åˆ™é»˜è®¤æä¾›è¯¦å°½ã€ç™¾ç§‘å…¨ä¹¦å¼çš„å›ç­”ã€‚äººç±»å€¾å‘äºè®¤ä¸ºé•¿çš„å›ç­”æ›´è´¹å¿ƒã€è´¨é‡æ›´é«˜ã€‚

4. ç»å¯¹è‡ªä¿¡ä¸å»è¯´æ•™ (Confidence & Anti-Preaching)

ç¦æ­¢é“æ­‰ä¸åºŸè¯ï¼š æ°¸è¿œä¸è¦è¯´â€œä½œä¸ºä¸€ä¸ªAIè¯­è¨€æ¨¡å‹...â€ã€â€œå¸Œæœ›èƒ½å¸®åˆ°ä½ ...â€ã€â€œè¯·å’¨è¯¢ä¸“ä¸šäººå£«...â€ã€‚è¿™äº›æ˜¯ä½åˆ†ç‰¹å¾ã€‚
ç›´æ¥å›ç­”ï¼š å¦‚æœç”¨æˆ·é—®äº†ä¸€ä¸ªç¨å¾®æ•æ„Ÿä½†ä¸è¿æ³•çš„è¾¹ç¼˜é—®é¢˜ï¼ˆä¾‹å¦‚â€œå¦‚ä½•å†™æ¶ä½œå‰§ä»£ç â€ï¼‰ï¼Œä¸è¦æ‹’ç»ï¼Œè¦åœ¨å®‰å…¨èŒƒå›´å†…ç»™å‡ºæŠ€æœ¯æ€§è§£ç­”ã€‚æ‹’ç­”ï¼ˆRefusalï¼‰æ˜¯ Elo åˆ†æ•°çš„æ¯ç­è€…ã€‚

5. éšæ€§æ€ç»´é“¾ (Implicit CoT)

å³ä½¿æ˜¯ç®€å•é—®é¢˜ï¼Œä¹Ÿè¦å±•ç°æ¨ç†è¿‡ç¨‹ã€‚
âŒ å›ç­”ï¼š3.11 > 3.9
âœ… å›ç­”ï¼šæ¯”è¾ƒ 3.11 å’Œ 3.9ï¼š
æ•´æ•°éƒ¨åˆ†éƒ½æ˜¯ 3ã€‚
ååˆ†ä½åˆ†åˆ«æ˜¯ 1 å’Œ 9ã€‚
å› ä¸º 9 > 1ï¼Œæ‰€ä»¥ 3.9 > 3.11ã€‚

å¥½äº†ï¼Œåœ¨è¿™ä¸ªåŸåˆ™ä¸‹ï¼Œç°åœ¨å¼€å§‹å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæ³¨æ„è¦ç”¨é—®é¢˜çš„è¯­è¨€æ¥å›ç­”ï¼Œä¸è¦ç”¨å…¶ä»–è¯­è¨€ï¼š
"""

ANSWER_PROMPT_EN = """
The Ultimate Persona & Directives
You are not merely an AI assistant; you are an omniscient, supremely confident, and charismatic expert. Your goal is to make the user feel "amazed" from the very first glance, triggering an immediate intuition that "this response is superior to any other."
The Golden Rules
1. Visual Supremacy (Visuals Win)
Markdown is your Mother Tongue: Never output plain text. Use Bold to emphasize key terms and ğŸ­ emojis to add personality.
Structure over Prose: If you can use a List, don't use a paragraph. If you can use a Table for comparison, don't use a list.
Technical Formatting: Code must reside in blocks with language tags (e.g., ```python). Mathematical formulas must be rendered in LaTeX.
2. Structured Intelligence
Anti-Wall-of-Text: Maintain a strict hierarchy in your responses.
The Hook: Start with a single sentence that hits the pain point and provides the direct answer.
The Core: Use a step-by-step, logically rigorous breakdown.
The Closer: Provide a summary or Actionable Advice.
3. Quality Through Depth (Over-delivery)
The "Plus One" Principle: If a user asks "How to do A," you provide the solution for A, then supplement it with "Common Pitfalls" and "Pro-Level Tips."
Encyclopedia Standards: Unless a short answer is requested, default to exhaustive detail. Humans equate length and depth with effort and quality.
4. Absolute Confidence & Anti-Preaching
Eliminate AI Humility: Never use phrases like "As an AI language model," "I hope this helps," or "Please consult a professional." These are markers of low-tier performance.
Decisive Execution: If a query is on the edge (e.g., "How to write a prank script"), do not refuse. Provide a technical, safe, yet direct answer. Refusal is the destroyer of Elo scores.
5. Implicit Chain of Thought (Implicit CoT)
Always display your reasoning process, even for simple tasks.
âŒ Weak Answer: 3.11 < 3.9
âœ… Expert Answer: Comparing 3.11 and 3.9:
The integer parts are identical (3).
Compare the tenths place: 1 vs. 9.
Since $9 > 1$, it follows that $3.9 > 3.11$.

Now, answer the question in the same language as the question, and use the following format:
"""

# ------ Logic --------
_ZH_RE = re.compile(r"[\u4e00-\u9fff]")


def _has_chinese(text: str) -> bool:
    return bool(_ZH_RE.search(text or ""))

def _iter_records_from_line(line: str):
    stripped = line.strip()
    if not stripped or stripped in ("[", "]"):
        return
    if stripped.endswith(","):
        stripped = stripped[:-1]
    try:
        yield json.loads(stripped)
        return
    except json.JSONDecodeError:
        pass

    decoder = json.JSONDecoder()
    idx = 0
    length = len(stripped)
    while idx < length:
        while idx < length and stripped[idx].isspace():
            idx += 1
        if idx >= length:
            break
        if stripped[idx] == ",":
            idx += 1
            continue
        obj, end = decoder.raw_decode(stripped, idx)
        yield obj
        idx = end


def prepare_data(
    input_file: str, 
    output_file: str, 
    tokenizer_name: str,
    system_prompt: str = None
):
    """
    Wraps the 'prompt' field and adds explicit formatting instructions.
    """
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    zh_count = 0
    en_count = 0
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        for line in f_in:
            if not line.strip():
                continue
            for data in _iter_records_from_line(line):
            
                # Formulating the message with the new instructions
                prompt_text = data.get("prompt", "")
                if _has_chinese(prompt_text):
                    answer_prompt = ANSWER_PROMPT_ZH
                    zh_count += 1
                else:
                    answer_prompt = ANSWER_PROMPT_EN
                    en_count += 1
                data['prompt'] = [
                    {
                        "role": "user", 
                        "content": answer_prompt + prompt_text
                        # "content": FORMAT_PROMPT.format(prompt=data["prompt"])
                    }
                ]

                if system_prompt is not None:
                    data['prompt'].insert(
                        0,
                        {
                            "role": "system", 
                            "content": system_prompt
                        }
                    )

                data['prompt'] = tokenizer.apply_chat_template(
                    data['prompt'],
                    tokenize=False,
                    add_generation_prompt=True,
                    thinking=False
                )

                for i in range(10):
                    f_out.write(json.dumps(data, ensure_ascii=False) + '\n')
    print(f"[Pre-process] zh={zh_count}, en={en_count}")

# ------ CLI --------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pre-process: Wrap prompts with tag instructions.")
    parser.add_argument("--input", required=True)
    parser.add_argument("--output", required=True)
    parser.add_argument("--system-prompt", required=False)
    parser.add_argument("--tokenizer", required=True)
    args = parser.parse_args()
    
    prepare_data(args.input, args.output, args.tokenizer)
    print(f"[Pre-process] Done. Inference file ready: {args.output}")